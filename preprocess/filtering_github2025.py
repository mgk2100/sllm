"""
GitHub 2025 ë°ì´í„°ì…‹ í•„í„°ë§ ìŠ¤í¬ë¦½íŠ¸
íŠ¹ì • ì–¸ì–´ì˜ ì½”ë“œ íŒŒì¼ë§Œ ìˆ˜ì§‘í•˜ì—¬ JSONìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
python filtering_github2025.py --languages python c cpp --sample_size 500 --output_path data/output/github2025.json
"""

from datasets import load_dataset
from argparse import ArgumentParser
import json
import os
from typing import List

# í—ˆìš©ëœ ì–¸ì–´ì™€ í•´ë‹¹ íŒŒì¼ í™•ì¥ì ë§¤í•‘
ALLOWED_LANGUAGES = {
    'python': ['.py'],
    'javascript': ['.js', '.jsx'],
    'typescript': ['.ts', '.tsx'],
    'java': ['.java'],
    'c': ['.c', '.h'],
    'cpp': ['.cpp', '.hpp', '.cc', '.hh', '.cxx', '.hxx'],
    'go': ['.go'],
    'rust': ['.rs'],
    'ruby': ['.rb'],
    'php': ['.php'],
    'swift': ['.swift'],
    'kotlin': ['.kt', '.kts'],
    'scala': ['.scala'],
    'r': ['.r', '.R'],
    'shell': ['.sh', '.bash'],
}


def get_language_from_filepath(file_path: str) -> str:
    """
    íŒŒì¼ ê²½ë¡œì—ì„œ í™•ì¥ìë¥¼ ì¶”ì¶œí•˜ì—¬ ì–¸ì–´ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤.
    
    Args:
        file_path: íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì–¸ì–´ ì´ë¦„ ë˜ëŠ” None (ë§¤ì¹­ë˜ì§€ ì•Šì„ ê²½ìš°)
    """
    # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ (ì†Œë¬¸ìë¡œ ë³€í™˜)
    ext = os.path.splitext(file_path.lower())[1]
    
    # í™•ì¥ìë¡œ ì–¸ì–´ ì°¾ê¸°
    for language, extensions in ALLOWED_LANGUAGES.items():
        if ext in extensions:
            return language
    
    return None


def parse_args():
    """ì»¤ë§¨ë“œë¼ì¸ ì¸ì íŒŒì‹±"""
    parser = ArgumentParser(description="GitHub 2025 ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì • ì–¸ì–´ ì½”ë“œ ìˆ˜ì§‘")
    
    parser.add_argument(
        "--languages",
        type=str,
        nargs='+',  # ë¦¬ìŠ¤íŠ¸ë¡œ ì—¬ëŸ¬ ì–¸ì–´ ë°›ê¸°
        required=True,
        help=f"ìˆ˜ì§‘í•  ì–¸ì–´ ëª©ë¡ (í—ˆìš©: {', '.join(ALLOWED_LANGUAGES.keys())})"
    )
    
    parser.add_argument(
        "--output_path",
        type=str,
        default="data/output/github2025.json",
        help="ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--sample_size",
        type=int,
        default=100,
        help="ìˆ˜ì§‘í•  ìƒ˜í”Œ ê°œìˆ˜"
    )
    
    return parser.parse_args()


def validate_languages(languages: List[str]):
    """
    ì…ë ¥ëœ ì–¸ì–´ê°€ í—ˆìš©ëœ ëª©ë¡ì— ìˆëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        languages: ê²€ì¦í•  ì–¸ì–´ ë¦¬ìŠ¤íŠ¸
        
    Raises:
        AssertionError: í—ˆìš©ë˜ì§€ ì•Šì€ ì–¸ì–´ê°€ í¬í•¨ëœ ê²½ìš°
    """
    invalid_languages = [lang for lang in languages if lang not in ALLOWED_LANGUAGES]
    
    assert len(invalid_languages) == 0, (
        f"í—ˆìš©ë˜ì§€ ì•Šì€ ì–¸ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {invalid_languages}\n"
        f"í—ˆìš©ëœ ì–¸ì–´ ëª©ë¡: {list(ALLOWED_LANGUAGES.keys())}"
    )


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    args = parse_args()
    
    # 1. ì–¸ì–´ ê²€ì¦
    print(f"ğŸ” ì…ë ¥ëœ ì–¸ì–´: {args.languages}")
    validate_languages(args.languages)
    print(f"âœ… ì–¸ì–´ ê²€ì¦ ì™„ë£Œ")
    
    # 2. í—ˆìš©ëœ í™•ì¥ì ëª©ë¡ ìƒì„±
    target_extensions = []
    for lang in args.languages:
        target_extensions.extend(ALLOWED_LANGUAGES[lang])
    print(f"ğŸ“ ëŒ€ìƒ í™•ì¥ì: {target_extensions}")
    
    # 3. ë°ì´í„°ì…‹ ë¡œë“œ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ)
    print(f"ğŸŒ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    streaming_data = load_dataset(
        "nick007x/github-code-2025", 
        streaming=True
    )
    
    # 4. í•„í„°ë§í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘
    print(f"ğŸ” í•„í„°ë§ ì‹œì‘ (ëª©í‘œ: {args.sample_size}ê°œ)")
    collected_data = []
    total_checked = 0
    
    try:
        for example in streaming_data['train']:
            total_checked += 1
            
            # íŒŒì¼ ê²½ë¡œì—ì„œ ì–¸ì–´ ì¶”ë¡ 
            if 'file_path' not in example:
                continue
                
            detected_language = get_language_from_filepath(example['file_path'])
            
            # ì›í•˜ëŠ” ì–¸ì–´ì¸ ê²½ìš° ìˆ˜ì§‘
            if detected_language in args.languages:
                # ë°ì´í„° êµ¬ì¡° ìœ ì§€: repo_id, size, file_path, content
                collected_data.append({
                    'repo_id': example.get('repo_id', 'unknown'),
                    'size': example.get('size', len(example.get('content', ''))),
                    'file_path': example['file_path'],
                    'content': example.get('content', '')
                })
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if len(collected_data) % 10 == 0:
                    print(f"  ğŸ“¦ ìˆ˜ì§‘ë¨: {len(collected_data)}/{args.sample_size} (ê²€ì‚¬: {total_checked}ê°œ)")
            
            # ëª©í‘œ ê°œìˆ˜ ë‹¬ì„± ì‹œ ì¢…ë£Œ
            if len(collected_data) >= args.sample_size:
                print(f"âœ… ëª©í‘œ ë‹¬ì„±! {len(collected_data)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                break
    finally:
        # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì •ë¦¬ (GIL ê´€ë ¨ ì—ëŸ¬ ë°©ì§€)
        del streaming_data
    
    # 5. ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
    print(f"  - ì´ ê²€ì‚¬í•œ íŒŒì¼: {total_checked}ê°œ")
    print(f"  - ìˆ˜ì§‘ëœ íŒŒì¼: {len(collected_data)}ê°œ")
    
    if len(collected_data) < args.sample_size:
        print(f"  âš ï¸ ëª©í‘œ({args.sample_size}ê°œ)ì— ë¯¸ë‹¬í–ˆì§€ë§Œ ê°€ëŠ¥í•œ ë§Œí¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    
    # ì–¸ì–´ë³„ í†µê³„
    language_counts = {}
    for data in collected_data:
        lang = get_language_from_filepath(data['file_path'])
        language_counts[lang] = language_counts.get(lang, 0) + 1
    
    print(f"\nğŸ“ˆ ì–¸ì–´ë³„ í†µê³„:")
    for lang, count in sorted(language_counts.items()):
        print(f"  - {lang}: {count}ê°œ")
    
    # 6. JSONìœ¼ë¡œ ì €ì¥
    os.makedirs(os.path.dirname(args.output_path), exist_ok=True)
    with open(args.output_path, "w", encoding="utf-8") as f:
        json.dump(collected_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {args.output_path}")


if __name__ == "__main__":
    main()